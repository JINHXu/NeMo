jos-MacBook-Pro:NeMo xujinghua$ python /Users/xujinghua/NeMo/examples/speaker_recognition/speaker_reco.py
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:54 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:58 experimental:28] Module <class 'nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2021-02-15 14:33:58 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text_dali.AudioToCharDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
################################################################################
### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk
###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)
###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)
################################################################################

[NeMo W 2021-02-15 14:33:58 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: "sox" backend is being deprecated. The default backend will be changed to "sox_io" backend in 0.8.0 and "sox" backend will be removed in 0.9.0. Please migrate to "sox_io" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.
      '"sox" backend is being deprecated. '
    
[NeMo W 2021-02-15 14:33:59 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.
    Use OmegaConf.to_yaml(cfg)
    
      category=UserWarning,
    
[NeMo I 2021-02-15 14:33:59 speaker_reco:83] Hydra config: name: SpeakerNet
    sample_rate: 16000
    repeat: 2
    dropout: 0.5
    separable: true
    n_filters: 512
    model:
      train_ds:
        manifest_filepath: /Users/xujinghua/NeMo/data/an4/wav/an4_clstk/train.json
        sample_rate: 16000
        labels: null
        batch_size: 64
        shuffle: true
        time_length: 8
        is_tarred: false
        tarred_audio_filepaths: null
        tarred_shard_strategy: scatter
      validation_ds:
        manifest_filepath: /Users/xujinghua/NeMo/data/an4/wav/an4_clstk/dev.json
        sample_rate: 16000
        labels: null
        batch_size: 128
        shuffle: false
        time_length: 8
      test_ds:
        manifest_filepath: /Users/xujinghua/NeMo/data/an4/wav/an4_clstk/dev.json
        sample_rate: 16000
        labels: null
        batch_size: 1
        shuffle: false
        time_length: 8
        embedding_dir: .
      preprocessor:
        _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
        normalize: per_feature
        window_size: 0.02
        sample_rate: 16000
        window_stride: 0.01
        window: hann
        features: 64
        n_fft: 512
        frame_splicing: 1
        dither: 1.0e-05
        stft_conv: false
      encoder:
        _target_: nemo.collections.asr.modules.ConvASREncoder
        feat_in: 64
        activation: relu
        conv_mask: true
        jasper:
        - filters: 512
          repeat: 1
          kernel:
          - 3
          stride:
          - 1
          dilation:
          - 1
          dropout: 0.5
          residual: true
          separable: true
        - filters: 512
          repeat: 2
          kernel:
          - 7
          stride:
          - 1
          dilation:
          - 1
          dropout: 0.5
          residual: true
          separable: true
        - filters: 512
          repeat: 2
          kernel:
          - 11
          stride:
          - 1
          dilation:
          - 1
          dropout: 0.5
          residual: true
          separable: true
        - filters: 512
          repeat: 2
          kernel:
          - 15
          stride:
          - 1
          dilation:
          - 1
          dropout: 0.5
          residual: true
          separable: true
        - filters: 1500
          repeat: 1
          kernel:
          - 1
          stride:
          - 1
          dilation:
          - 1
          dropout: 0.0
          residual: false
          separable: true
      decoder:
        _target_: nemo.collections.asr.modules.SpeakerDecoder
        feat_in: 1500
        num_classes: 74
        pool_mode: xvector
        emb_sizes: 512,512
        angular: false
      loss:
        scale: 30
        margin: 0.2
      optim:
        name: novograd
        lr: 0.006
        args:
          name: auto
          betas:
          - 0.95
          - 0.5
          weight_decay: 0.001
        sched:
          name: CosineAnnealing
          iters_per_batch: 1
          max_steps: null
          args:
            name: auto
            warmup_steps: null
            warmup_ratio: 0.1
            min_lr: 0.0
            last_epoch: -1
    trainer:
      gpus: 0
      max_epochs: 5
      max_steps: null
      num_nodes: 1
      accelerator: null
      accumulate_grad_batches: 1
      amp_level: O0
      deterministic: true
      checkpoint_callback: false
      logger: false
      log_every_n_steps: 1
      val_check_interval: 1.0
    exp_manager:
      exp_dir: null
      name: SpeakerNet
      create_tensorboard_logger: true
      create_checkpoint_callback: true
    
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
[NeMo I 2021-02-15 14:33:59 exp_manager:183] Experiments will be logged at /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59
[NeMo I 2021-02-15 14:33:59 exp_manager:519] TensorboardLogger has been set up
[NeMo W 2021-02-15 14:33:59 exp_manager:562] trainer had a weights_save_path of cwd(). This was ignored.
[NeMo I 2021-02-15 14:33:59 collections:256] Filtered duration for loading collection is 0.000000.
[NeMo I 2021-02-15 14:33:59 collections:259] # 853 files loaded accounting to # 74 labels
[NeMo I 2021-02-15 14:33:59 audio_to_label:97] Timelength considered for collate func is 8
[NeMo I 2021-02-15 14:33:59 collections:256] Filtered duration for loading collection is 0.000000.
[NeMo I 2021-02-15 14:33:59 collections:259] # 95 files loaded accounting to # 74 labels
[NeMo I 2021-02-15 14:33:59 audio_to_label:97] Timelength considered for collate func is 8
[NeMo I 2021-02-15 14:33:59 collections:256] Filtered duration for loading collection is 0.000000.
[NeMo I 2021-02-15 14:33:59 collections:259] # 95 files loaded accounting to # 74 labels
[NeMo I 2021-02-15 14:33:59 audio_to_label:97] Timelength considered for collate func is 8
[NeMo I 2021-02-15 14:33:59 features:241] PADDING: 16
[NeMo I 2021-02-15 14:33:59 features:258] STFT using torch
[NeMo I 2021-02-15 14:33:59 label_models:89] Training with Softmax-CrossEntropy loss
[NeMo I 2021-02-15 14:33:59 modelPT:597] Optimizer config = Novograd (
    Parameter Group 0
        amsgrad: False
        betas: [0.95, 0.5]
        eps: 1e-08
        grad_averaging: False
        lr: 0.006
        weight_decay: 0.001
    )
[NeMo I 2021-02-15 14:33:59 lr_scheduler:562] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x15cd7d0b8>" 
    will be used during training (effective maximum steps = 70) - 
    Parameters : 
    (last_epoch: -1
    warmup_steps: null
    warmup_ratio: null
    min_lr: 0.0
    max_steps: 70
    )

  | Name         | Type                              | Params
-------------------------------------------------------------------
0 | preprocessor | AudioToMelSpectrogramPreprocessor | 0     
1 | encoder      | ConvASREncoder                    | 3.2 M 
2 | decoder      | SpeakerDecoder                    | 1.8 M 
3 | loss         | CrossEntropyLoss                  | 0     
4 | _accuracy    | TopKClassificationAccuracy        | 0     
[NeMo W 2021-02-15 14:33:59 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
Validation sanity check: 0it [00:00, ?it/s][NeMo W 2021-02-15 14:34:00 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  ../aten/src/ATen/native/SpectralOps.cpp:653.)
      normalized, onesided, return_complex)
    
[NeMo W 2021-02-15 14:34:00 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  ../aten/src/ATen/native/SpectralOps.cpp:590.)
      normalized, onesided, return_complex)
    
Validation sanity check: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.08s/it][NeMo I 2021-02-15 14:34:10 label_models:211] val_loss: 4.305
[NeMo W 2021-02-15 14:34:10 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
      warnings.warn(*args, **kwargs)
    
[NeMo W 2021-02-15 14:34:10 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
Epoch 0:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 14/15 [05:13<00:22, 22.36s/it, loss=4.670, v_num=3-59[
NeMo I 2021-02-15 14:39:34 label_models:211] val_loss: 6.398███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.96s/it]
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:24<00:00, 21.61s/it, loss=4.670, v_num=3-59E
poch 0: val_loss reached 6.39793 (best 6.39793), saving model to /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59/checkpoints/SpeakerNet---val_loss=6.40-epoch=0.ckpt as top 3
Epoch 1:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 14/15 [05:01<00:21, 21.56s/it, loss=4.420, v_num=3-59[
NeMo I 2021-02-15 14:44:47 label_models:211] val_loss: 25.174██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.44s/it]
Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:12<00:00, 20.82s/it, loss=4.420, v_num=3-59E
poch 1: val_loss reached 25.17365 (best 6.39793), saving model to /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59/checkpoints/SpeakerNet---val_loss=25.17-epoch=1.ckpt as top 3
Epoch 2:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 14/15 [05:01<00:21, 21.56s/it, loss=4.208, v_num=3-59[
NeMo I 2021-02-15 14:49:59 label_models:211] val_loss: 34.951██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.34s/it]
Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:12<00:00, 20.81s/it, loss=4.208, v_num=3-59E
poch 2: val_loss reached 34.95110 (best 6.39793), saving model to /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59/checkpoints/SpeakerNet---val_loss=34.95-epoch=2.ckpt as top 3
Epoch 3:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 14/15 [05:02<00:21, 21.60s/it, loss=4.118, v_num=3-59[
NeMo I 2021-02-15 14:55:12 label_models:211] val_loss: 29.749██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.23s/it]
Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:12<00:00, 20.85s/it, loss=4.118, v_num=3-59E
poch 3: val_loss reached 29.74898 (best 6.39793), saving model to /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59/checkpoints/SpeakerNet---val_loss=29.75-epoch=3.ckpt as top 3
Epoch 4:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████▊       | 14/15 [05:55<00:25, 25.41s/it, loss=4.072, v_num=3-59[
NeMo I 2021-02-15 15:01:20 label_models:211] val_loss: 29.221██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.77s/it]
Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [06:07<00:00, 24.51s/it, loss=4.072, v_num=3-59E
poch 4: val_loss reached 29.22142 (best 6.39793), saving model to /Users/xujinghua/NeMo/nemo_experiments/SpeakerNet/2021-02-15_14-33-59/checkpoints/SpeakerNet---val_loss=29.22-epoch=4.ckpt as top 3
Saving latest checkpoint...
Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [06:07<00:00, 24.52s/it, loss=4.072, v_num=3-59]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
[NeMo W 2021-02-15 15:01:22 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
      warnings.warn(*args, **kwargs)
    
Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:06<00:00, 18.09it/s][NeMo I 2021-02-15 15:01:29 label_models:244] test_loss: 29.241
[NeMo W 2021-02-15 15:01:29 nemo_logging:349] /Users/xujinghua/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The testing_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
      warnings.warn(*args, **kwargs)
    
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'test_epoch_accuracy_top@1': tensor([0.0105]), 'test_loss': tensor(29.2409)}
--------------------------------------------------------------------------------
DATALOADER:1 TEST RESULTS
{'test_acc_top_k': [tensor([0.0105])], 'test_loss': tensor(29.2409)}
--------------------------------------------------------------------------------
Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:06<00:00, 14.84it/s]
[{'test_loss': 29.240917205810547, 'test_epoch_accuracy_top@1': 0.010526316240429878}, {'test_loss': 29.240917205810547, 'test_acc_top_k': [tensor([0.0105])]}]